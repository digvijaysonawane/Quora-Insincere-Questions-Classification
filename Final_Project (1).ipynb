{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import nltk\n",
    "import xgboost as xgb\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset :  (1306122, 3)\n",
      "Test Dataset :  (375806, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset : \", train_df.shape)\n",
    "print(\"Test Dataset : \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Sincere  Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "g = sns.countplot(train_df.target, palette='gist_rainbow')\n",
    "g.set_xticklabels(['Sincere', 'Insincere'])\n",
    "g.set_yticklabels([])\n",
    "\n",
    "def value_bars(axs):\n",
    "    def single_plot(ax):        \n",
    "        for p in ax.patches:\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            _y = p.get_y() + p.get_height()\n",
    "            value = '{:.0f}'.format(p.get_height())\n",
    "            ax.text(_x, _y, value, ha=\"center\") \n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            single_plot(ax)\n",
    "    else:\n",
    "        single_plot(axs)\n",
    "value_bars(ax)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.title('Distribution of Questions', fontsize=35)\n",
    "plt.tick_params(axis='x', which='major', labelsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the media spend months telling us Hillary Clinton was in perfect health as she stumbled around, coughed, fainted, and for months now has tried to sell the public that Trump is sick and has dementia?\n",
      "As the Christian bible is based on hearsay, is it right to use it in a court of law to take the oath?\n",
      "Does Indian girls like forceful sex?\n",
      "How would you feel if Donald Trump got shot in the head?\n",
      "How many Jews does it take to screw in a lightbulb?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.sample(train_df.index[train_df.target == 1].tolist(), 5)\n",
    "for i in index:\n",
    "    print(train_df.iloc[i, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary for special tokens\n",
    "SPECIAL_TOKENS = {\n",
    "    'quoted': 'quoted_item',\n",
    "    'non-ascii': 'non_ascii_word',\n",
    "    'undefined': 'something'\n",
    "}\n",
    "\n",
    "\n",
    "# defining a function clean to clean the data for easier modeling\n",
    "def clean(text, stem_words=True):\n",
    "    import re\n",
    "    from string import punctuation\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    def pad_str(s):\n",
    "        return ' '+s+' '\n",
    "    \n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "\n",
    "    # stops = set(stopwords.words(\"english\"))\n",
    "    # Clean the text, with the option to stem words.\n",
    "    \n",
    "    # Empty question\n",
    "    if type(text) != str or text == '':\n",
    "        return ''\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(\"\\'s\", \" \", text) \n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"can't\", \"can not\", text)\n",
    "    text = re.sub(\"n't\", \" not \", text)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(\\d+)(kK)\", \" \\g<1>000 \", text)\n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "\n",
    "    # add padding to punctuations and special chars, we still need them later\n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "\n",
    "    # replace non-ascii word with special word\n",
    "    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text)\n",
    "    \n",
    "    # indian dollar\n",
    "    text = re.sub(\"(?<=[0-9])rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\" rs(?=[0-9])\", \" rs \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # clean text rules get from : https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" india \", \" India \", text)\n",
    "    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n",
    "    text = re.sub(r\" china \", \" China \", text)\n",
    "    text = re.sub(r\" chinese \", \" Chinese \", text) \n",
    "    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" \\0rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gps \", \" GPS \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gst \", \" GST \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dna \", \" DNA \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" III \", \" 3 \", text)\n",
    "    text = re.sub(r\" banglore \", \" Banglore \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # replace the float numbers with a random number, it will be parsed as number afterward, and also been replaced with\n",
    "    # word \"number\"\n",
    "    text = re.sub('[0-9]+\\.[0-9]+', \" 87 \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation]).lower()\n",
    "\n",
    "    # Return a list of words\n",
    "    return text\n",
    "\n",
    "\n",
    "# applying clean function on training data\n",
    "train_df['question_text'] = train_df['question_text'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, ngram_range=(1,3),\n",
    "                        strip_accents='unicode',\n",
    "                        lowercase =True, analyzer='word',\n",
    "                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n",
    "                        stop_words = 'english',tokenizer=word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=5, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents='unicode',\n",
       "                sublinear_tf=True, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function word_tokenize at 0x000001D41DA31730>,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train_df.question_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1306122x234530 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10058097 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectorized = vectorizer.transform(train_df.question_text.values)\n",
    "train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectorized = vectorizer.transform(test_df.question_text.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val=train_test_split(train_vectorized,train_df.target.values,test_size=0.1,stratify =train_df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=5, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.01,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC(dual=True,C=5,penalty='l2',max_iter=1000,tol=0.01)\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_preds=svc.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score  0.8192840221885438\n",
      "Precision Score  0.8810703039701252\n",
      "Recall Score  0.7655955671052812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score,recall_score\n",
    "print(\"F1 Score \" , f1_score(y_train,svc_preds))\n",
    "print(\"Precision Score \" ,precision_score(y_train,svc_preds))\n",
    "print(\"Recall Score \" ,recall_score(y_train,svc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting.......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 score:  0.6450546290971824\n",
      "Training Accuracy:  0.9524717024717024\n",
      "Testing F1 score:  0.624108995759271\n",
      "Testing Accuracy:  0.9489667168914532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split to train and val\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=2018)\n",
    "\n",
    "# split training data to train and test data\n",
    "train_data, test_data = train_test_split(train_df, test_size=float(1.0/8), random_state=2018)\n",
    "\n",
    "# storing question_text and target columns in different lists\n",
    "train_text = train_data['question_text']\n",
    "valid_text = val_data['question_text']\n",
    "test_text = test_data['question_text']\n",
    "train_target = train_data['target']\n",
    "valid_target = val_data['target']\n",
    "test_target = test_data['target']\n",
    "all_text = train_text.append(train_text)\n",
    "\n",
    "# applying TFIDF and count vectorization to question text and target values for each training and testing data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(all_text)\n",
    "\n",
    "train_text_features_tf = tfidf_vectorizer.ktransform(train_text)\n",
    "test_text_features_tf = tfidf_vectorizer.transform(test_text)\n",
    "\n",
    "# using kfold technique for fitting and predicting using Logistic Regression\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "test_preds = 0\n",
    "oof_preds = np.zeros([train_data.shape[0],])\n",
    "\n",
    "classifier1 = LogisticRegression()\n",
    "print('fitting.......')\n",
    "classifier1.fit(train_text_features_tf,train_target)\n",
    "pred_train=classifier1.predict_proba(train_text_features_tf)[:,1]\n",
    "# training data F1 score\n",
    "pred_train = np.where(pred_train > 0.25, 1, 0)\n",
    "print(\"Training F1 score: \",f1_score(train_target, pred_train))\n",
    "\n",
    "# training data accuracy score\n",
    "print(\"Training Accuracy: \",accuracy_score(train_target, pred_train))\n",
    "\n",
    "pred_test=classifier1.predict_proba(test_text_features_tf)[:,1]\n",
    "pred_test = np.where(pred_test > 0.25, 1, 0)\n",
    "print(\"Testing F1 score: \",f1_score(test_target, pred_test))\n",
    "\n",
    "# training data accuracy score\n",
    "print(\"Testing Accuracy: \",accuracy_score(test_target, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 score:  0.5487336305239224\n",
      "Training Accuracy:  0.9204589204589204\n",
      "Testing F1 score:  0.5434520858417735\n",
      "Testing Accuracy:  0.9210368355934487\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count_vectorizer1 = CountVectorizer()\n",
    "count_vectorizer1.fit(all_text)\n",
    "\n",
    "train_text_features_cv = count_vectorizer1.transform(train_text)\n",
    "test_text_features_cv = count_vectorizer1.transform(test_text)\n",
    "\n",
    "# using kfold technique for fitting and predicting using Naive Bayes\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "test_preds = 0\n",
    "oof_preds = np.zeros([train_data.shape[0],])\n",
    "\n",
    "\n",
    "\n",
    "classifier1 = MultinomialNB()\n",
    "classifier1.fit(train_text_features_cv,train_target)\n",
    "pred_train=classifier1.predict_proba(train_text_features_cv)[:,1]\n",
    "# training data F1 score\n",
    "pred_train = np.where(pred_train > 0.25, 1, 0)\n",
    "print(\"Training F1 score: \",f1_score(train_target, pred_train))\n",
    "\n",
    "# training data accuracy score\n",
    "print(\"Training Accuracy: \",accuracy_score(train_target, pred_train))\n",
    "\n",
    "pred_test=classifier1.predict_proba(test_text_features_cv)[:,1]\n",
    "pred_test = np.where(pred_test > 0.25, 1, 0)\n",
    "print(\"Testing F1 score: \",f1_score(test_target, pred_test))\n",
    "\n",
    "# training data accuracy score\n",
    "print(\"Testing Accuracy: \",accuracy_score(test_target, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# XGboost related\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import csr_matrix, hstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data to validation\n",
    "train_df, val_df = train_test_split(train_df, train_size=0.9, random_state=235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fill missing and get the values\n"
     ]
    }
   ],
   "source": [
    "print('fill missing and get the values')\n",
    "# fill missing and get the values\n",
    "X_train = train_df[\"question_text\"].fillna(\"na_\").values\n",
    "X_val = val_df[\"question_text\"].fillna(\"na_\").values\n",
    "X_test = test_df[\"question_text\"].fillna(\"na_\").values\n",
    "\n",
    "y_train = train_df['target'].values\n",
    "y_val = val_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training data:  (1175509,)\n"
     ]
    }
   ],
   "source": [
    "print('size of training data: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vector = TfidfVectorizer(\n",
    "    ngram_range=(2,4),\n",
    "    max_features=20000,\n",
    "    stop_words='english',\n",
    "    analyzer='char_wb',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    strip_accents='unicode',\n",
    "    sublinear_tf=True, \n",
    "    max_df=0.98,\n",
    "    min_df=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the data into TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:520: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.98,\n",
       "                max_features=20000, min_df=2, ngram_range=(2, 4), norm='l2',\n",
       "                preprocessor=None, smooth_idf=True, stop_words='english',\n",
       "                strip_accents='unicode', sublinear_tf=True,\n",
       "                token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vector.fit(X_train[:85000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranform the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_vector = char_vector.transform(X_train).tocsr()\n",
    "valid_char_vector = char_vector.transform(X_val).tocsr()\n",
    "test_char_vector = char_vector.transform(X_test).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = list(X_train) + list(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = TfidfVectorizer(\n",
    "    ngram_range=(1,1), \n",
    "    max_features=9000,\n",
    "    sublinear_tf=True, \n",
    "    strip_accents='unicode', \n",
    "    analyzer='word', \n",
    "    token_pattern=\"\\w{1,}\", \n",
    "    stop_words=\"english\",\n",
    "    max_df=0.95,\n",
    "    min_df=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.95, max_features=9000,\n",
       "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents='unicode',\n",
       "                sublinear_tf=True, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_vector = word_vector.transform(X_train).tocsr()\n",
    "valid_word_vector = word_vector.transform(X_val).tocsr()\n",
    "test_word_vector = word_vector.transform(X_test).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_text\n",
    "del X_train\n",
    "del X_val\n",
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [train_df, val_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistake_list = ['colour', 'centre', 'favourite', 'travelling', 'counselling', 'theatre', 'cancelled', 'labour', 'organisation', 'wwii', 'citicise', 'youtu ', 'youtube ', 'Qoura', 'sallary', 'Whta', 'narcisist', 'howdo', 'whatare', 'howcan', 'howmuch', 'howmany', 'whydo', 'doI', 'theBest', 'howdoes', 'mastrubation', 'mastrubate', \"mastrubating\", 'pennis', 'Etherium', 'narcissit', 'bigdata', '2k17', '2k18', 'qouta', 'exboyfriend', 'airhostess', 'whst', 'watsapp', 'demonitisation', 'demonitization', 'demonetisation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "    for dataframe in data:\n",
    "        dataframe[\"text_size\"] = dataframe[\"question_text\"].apply(len).astype('uint16')\n",
    "        dataframe[\"capital_size\"] = dataframe[\"question_text\"].apply(lambda x: sum(1 for c in x if c.isupper())).astype('uint16')\n",
    "        dataframe[\"capital_rate\"] = dataframe.apply(lambda x: float(x[\"capital_size\"]) / float(x[\"text_size\"]), axis=1).astype('float16')\n",
    "        dataframe[\"exc_count\"] = dataframe[\"question_text\"].apply(lambda x: x.count(\"!\")).astype('uint16')\n",
    "        dataframe[\"quetion_count\"] = dataframe[\"question_text\"].apply(lambda x: x.count(\"?\")).astype('uint16')\n",
    "        dataframe[\"unq_punctuation_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(p) for p in '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²')).astype('uint16')\n",
    "        dataframe[\"punctuation_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(p) for p in '.,;:^_`')).astype('uint16')\n",
    "        dataframe[\"symbol_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(p) for p in '*&$%')).astype('uint16')\n",
    "        dataframe[\"words_count\"] = dataframe[\"question_text\"].apply(lambda x: len(x.split())).astype('uint16')\n",
    "        dataframe[\"unique_words\"] = dataframe[\"question_text\"].apply(lambda x: (len(set(1 for w in x.split())))).astype('uint16')\n",
    "        dataframe[\"unique_rate\"] = dataframe[\"unique_words\"] / dataframe[\"words_count\"]\n",
    "        dataframe[\"word_max_length\"] = dataframe[\"question_text\"].apply(lambda x: max([len(word) for word in x.split()]) ).astype('uint16')\n",
    "        dataframe[\"mistake_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(w) for w in mistake_list)).astype('uint16')\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"text_size\", \"capital_size\", \"capital_rate\", \"exc_count\", \"quetion_count\", \"unq_punctuation_count\", \"punctuation_count\", \"symbol_count\", \"words_count\", \"unique_words\", \"unique_rate\", \"word_max_length\", \"mistake_count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = csr_matrix(train_df[feature_cols].values)\n",
    "X_val = csr_matrix(val_df[feature_cols].values)\n",
    "X_test = csr_matrix(test_df[feature_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del val_df\n",
    "del train_df\n",
    "del test_df\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = hstack([X_train, train_word_vector, train_char_vector])\n",
    "input_valid = hstack([X_val, valid_word_vector, valid_char_vector])\n",
    "input_test = hstack([X_test, test_word_vector, test_char_vector])\n",
    "\n",
    "#print('input_train: ', input_train)\n",
    "train_word_vector = None\n",
    "train_char_vector = None\n",
    "valid_word_vector = None\n",
    "valid_char_vector = None\n",
    "test_word_vector = None\n",
    "test_char_vector = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xgb(train_X, train_y, valid_X, valid_y=None, subsample=0.75):\n",
    "\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "    if valid_y is not None:\n",
    "        xgvalid = xgb.DMatrix(valid_X, label=valid_y)\n",
    "    else:\n",
    "        xgvalid = None\n",
    "    \n",
    "    model_params = {}\n",
    "    # binary 0 or 1\n",
    "    model_params['objective'] = 'binary:logistic'\n",
    "    # eta is the learning_rate, [default=0.3]\n",
    "    model_params['eta'] = 0.3\n",
    "    # depth of the tree, deeper more complex.\n",
    "    model_params['max_depth'] = 6\n",
    "    # 0 [default] print running messages, 1 means silent mode\n",
    "    model_params['silent'] = 1\n",
    "    model_params['eval_metric'] = 'auc'\n",
    "    # will give up further partitioning [default=1]\n",
    "    model_params['min_child_weight'] = 1\n",
    "    # subsample ratio for the training instance\n",
    "    model_params['subsample'] = subsample\n",
    "    # subsample ratio of columns when constructing each tree\n",
    "    model_params['colsample_bytree'] = subsample\n",
    "    # random seed\n",
    "    model_params['seed'] = 2018\n",
    "    # imbalance data ratio\n",
    "    #model_params['scale_pos_weight'] = \n",
    "    \n",
    "    # convert params to list\n",
    "    model_params = list(model_params.items())\n",
    "    \n",
    "    return xgtrain, xgvalid, model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(xgtrain, xgvalid, model_params, num_rounds=500, patience=20):\n",
    "    \n",
    "    if xgvalid is not None:\n",
    "        # watchlist what information should be printed. specify validation monitoring\n",
    "        watchlist = [ (xgtrain, 'train'), (xgvalid, 'test') ]\n",
    "        #early_stopping_rounds = stop if performance does not improve for k rounds\n",
    "        model = xgb.train(model_params, xgtrain, num_rounds, watchlist, early_stopping_rounds=patience)\n",
    "    else:\n",
    "        model = xgb.train(model_params, xgtrain, num_rounds)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train the model\n",
      "[0]\ttrain-auc:0.763213\ttest-auc:0.758599\n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 20 rounds.\n",
      "[1]\ttrain-auc:0.774622\ttest-auc:0.770296\n",
      "[2]\ttrain-auc:0.819154\ttest-auc:0.817774\n",
      "[3]\ttrain-auc:0.850749\ttest-auc:0.848415\n",
      "[4]\ttrain-auc:0.858351\ttest-auc:0.857354\n",
      "[5]\ttrain-auc:0.868813\ttest-auc:0.867689\n",
      "[6]\ttrain-auc:0.876416\ttest-auc:0.87376\n",
      "[7]\ttrain-auc:0.879338\ttest-auc:0.87601\n",
      "[8]\ttrain-auc:0.883581\ttest-auc:0.880505\n",
      "[9]\ttrain-auc:0.890073\ttest-auc:0.886349\n",
      "[10]\ttrain-auc:0.894618\ttest-auc:0.890512\n",
      "[11]\ttrain-auc:0.898855\ttest-auc:0.894402\n",
      "[12]\ttrain-auc:0.901833\ttest-auc:0.896695\n",
      "[13]\ttrain-auc:0.904581\ttest-auc:0.899385\n",
      "[14]\ttrain-auc:0.908083\ttest-auc:0.902955\n",
      "[15]\ttrain-auc:0.910071\ttest-auc:0.905497\n",
      "[16]\ttrain-auc:0.91225\ttest-auc:0.907317\n",
      "[17]\ttrain-auc:0.913448\ttest-auc:0.908553\n",
      "[18]\ttrain-auc:0.915456\ttest-auc:0.910847\n",
      "[19]\ttrain-auc:0.917346\ttest-auc:0.912949\n",
      "[20]\ttrain-auc:0.918295\ttest-auc:0.913828\n",
      "[21]\ttrain-auc:0.919343\ttest-auc:0.914957\n",
      "[22]\ttrain-auc:0.920794\ttest-auc:0.916136\n",
      "[23]\ttrain-auc:0.922914\ttest-auc:0.91844\n",
      "[24]\ttrain-auc:0.924307\ttest-auc:0.919541\n",
      "[25]\ttrain-auc:0.925577\ttest-auc:0.920815\n",
      "[26]\ttrain-auc:0.926678\ttest-auc:0.921663\n",
      "[27]\ttrain-auc:0.927595\ttest-auc:0.922499\n",
      "[28]\ttrain-auc:0.928438\ttest-auc:0.923125\n",
      "[29]\ttrain-auc:0.929216\ttest-auc:0.923568\n",
      "[30]\ttrain-auc:0.929787\ttest-auc:0.924116\n",
      "[31]\ttrain-auc:0.930536\ttest-auc:0.924694\n",
      "[32]\ttrain-auc:0.931251\ttest-auc:0.925289\n",
      "[33]\ttrain-auc:0.931958\ttest-auc:0.925878\n",
      "[34]\ttrain-auc:0.932619\ttest-auc:0.926512\n",
      "[35]\ttrain-auc:0.933475\ttest-auc:0.927051\n",
      "[36]\ttrain-auc:0.934159\ttest-auc:0.927837\n",
      "[37]\ttrain-auc:0.934867\ttest-auc:0.928451\n",
      "[38]\ttrain-auc:0.935695\ttest-auc:0.929172\n",
      "[39]\ttrain-auc:0.936107\ttest-auc:0.92957\n",
      "[40]\ttrain-auc:0.936526\ttest-auc:0.929906\n",
      "[41]\ttrain-auc:0.937108\ttest-auc:0.930484\n",
      "[42]\ttrain-auc:0.937598\ttest-auc:0.930945\n",
      "[43]\ttrain-auc:0.938058\ttest-auc:0.931357\n",
      "[44]\ttrain-auc:0.938543\ttest-auc:0.931671\n",
      "[45]\ttrain-auc:0.938945\ttest-auc:0.932092\n",
      "[46]\ttrain-auc:0.939363\ttest-auc:0.932495\n",
      "[47]\ttrain-auc:0.939891\ttest-auc:0.932957\n",
      "[48]\ttrain-auc:0.940277\ttest-auc:0.933087\n",
      "[49]\ttrain-auc:0.94072\ttest-auc:0.933338\n",
      "[50]\ttrain-auc:0.941022\ttest-auc:0.933608\n",
      "[51]\ttrain-auc:0.941476\ttest-auc:0.933919\n",
      "[52]\ttrain-auc:0.941793\ttest-auc:0.934204\n",
      "[53]\ttrain-auc:0.942091\ttest-auc:0.934433\n",
      "[54]\ttrain-auc:0.942578\ttest-auc:0.934751\n",
      "[55]\ttrain-auc:0.943069\ttest-auc:0.935117\n",
      "[56]\ttrain-auc:0.943357\ttest-auc:0.935307\n",
      "[57]\ttrain-auc:0.943734\ttest-auc:0.935694\n",
      "[58]\ttrain-auc:0.944013\ttest-auc:0.935902\n",
      "[59]\ttrain-auc:0.944291\ttest-auc:0.935946\n",
      "[60]\ttrain-auc:0.94462\ttest-auc:0.936146\n",
      "[61]\ttrain-auc:0.94488\ttest-auc:0.936298\n",
      "[62]\ttrain-auc:0.945174\ttest-auc:0.936549\n",
      "[63]\ttrain-auc:0.945506\ttest-auc:0.936772\n",
      "[64]\ttrain-auc:0.945783\ttest-auc:0.936997\n",
      "[65]\ttrain-auc:0.94604\ttest-auc:0.937195\n",
      "[66]\ttrain-auc:0.94634\ttest-auc:0.937428\n",
      "[67]\ttrain-auc:0.946606\ttest-auc:0.937635\n",
      "[68]\ttrain-auc:0.946857\ttest-auc:0.93783\n",
      "[69]\ttrain-auc:0.947138\ttest-auc:0.938022\n",
      "[70]\ttrain-auc:0.947337\ttest-auc:0.93822\n",
      "[71]\ttrain-auc:0.947653\ttest-auc:0.938488\n",
      "[72]\ttrain-auc:0.947846\ttest-auc:0.938669\n",
      "[73]\ttrain-auc:0.948185\ttest-auc:0.938933\n",
      "[74]\ttrain-auc:0.948425\ttest-auc:0.939073\n",
      "[75]\ttrain-auc:0.948614\ttest-auc:0.93922\n",
      "[76]\ttrain-auc:0.948835\ttest-auc:0.939387\n",
      "[77]\ttrain-auc:0.949044\ttest-auc:0.939548\n",
      "[78]\ttrain-auc:0.949196\ttest-auc:0.939662\n",
      "[79]\ttrain-auc:0.949371\ttest-auc:0.939815\n",
      "[80]\ttrain-auc:0.949589\ttest-auc:0.939812\n",
      "[81]\ttrain-auc:0.949786\ttest-auc:0.939952\n",
      "[82]\ttrain-auc:0.950011\ttest-auc:0.940116\n",
      "[83]\ttrain-auc:0.950239\ttest-auc:0.940373\n",
      "[84]\ttrain-auc:0.950584\ttest-auc:0.940561\n",
      "[85]\ttrain-auc:0.950816\ttest-auc:0.940677\n",
      "[86]\ttrain-auc:0.951028\ttest-auc:0.940806\n",
      "[87]\ttrain-auc:0.95123\ttest-auc:0.940942\n",
      "[88]\ttrain-auc:0.95145\ttest-auc:0.941008\n",
      "[89]\ttrain-auc:0.9516\ttest-auc:0.941095\n",
      "[90]\ttrain-auc:0.951817\ttest-auc:0.941261\n",
      "[91]\ttrain-auc:0.95204\ttest-auc:0.941485\n",
      "[92]\ttrain-auc:0.95224\ttest-auc:0.941607\n",
      "[93]\ttrain-auc:0.952409\ttest-auc:0.941652\n",
      "[94]\ttrain-auc:0.952586\ttest-auc:0.941752\n",
      "[95]\ttrain-auc:0.95279\ttest-auc:0.941881\n",
      "[96]\ttrain-auc:0.952924\ttest-auc:0.94192\n",
      "[97]\ttrain-auc:0.953075\ttest-auc:0.942035\n",
      "[98]\ttrain-auc:0.953204\ttest-auc:0.942179\n",
      "[99]\ttrain-auc:0.953419\ttest-auc:0.942242\n",
      "[100]\ttrain-auc:0.953597\ttest-auc:0.942357\n",
      "[101]\ttrain-auc:0.953716\ttest-auc:0.942408\n",
      "[102]\ttrain-auc:0.953882\ttest-auc:0.942497\n",
      "[103]\ttrain-auc:0.953991\ttest-auc:0.942604\n",
      "[104]\ttrain-auc:0.954275\ttest-auc:0.942851\n",
      "[105]\ttrain-auc:0.954442\ttest-auc:0.943042\n",
      "[106]\ttrain-auc:0.954619\ttest-auc:0.943161\n",
      "[107]\ttrain-auc:0.954742\ttest-auc:0.94324\n",
      "[108]\ttrain-auc:0.954898\ttest-auc:0.943353\n",
      "[109]\ttrain-auc:0.955006\ttest-auc:0.943474\n",
      "[110]\ttrain-auc:0.955133\ttest-auc:0.943602\n",
      "[111]\ttrain-auc:0.955313\ttest-auc:0.943658\n",
      "[112]\ttrain-auc:0.95546\ttest-auc:0.943722\n",
      "[113]\ttrain-auc:0.9556\ttest-auc:0.94379\n",
      "[114]\ttrain-auc:0.955804\ttest-auc:0.943929\n",
      "[115]\ttrain-auc:0.955921\ttest-auc:0.944025\n",
      "[116]\ttrain-auc:0.956067\ttest-auc:0.944098\n",
      "[117]\ttrain-auc:0.956186\ttest-auc:0.944191\n",
      "[118]\ttrain-auc:0.956323\ttest-auc:0.944307\n",
      "[119]\ttrain-auc:0.956466\ttest-auc:0.944413\n",
      "[120]\ttrain-auc:0.956651\ttest-auc:0.944531\n",
      "[121]\ttrain-auc:0.956804\ttest-auc:0.944615\n",
      "[122]\ttrain-auc:0.956915\ttest-auc:0.944698\n",
      "[123]\ttrain-auc:0.957082\ttest-auc:0.944815\n",
      "[124]\ttrain-auc:0.957219\ttest-auc:0.944934\n",
      "[125]\ttrain-auc:0.957408\ttest-auc:0.944992\n",
      "[126]\ttrain-auc:0.957504\ttest-auc:0.945089\n",
      "[127]\ttrain-auc:0.957619\ttest-auc:0.945196\n",
      "[128]\ttrain-auc:0.957755\ttest-auc:0.945371\n",
      "[129]\ttrain-auc:0.957873\ttest-auc:0.945413\n",
      "[130]\ttrain-auc:0.957981\ttest-auc:0.945454\n",
      "[131]\ttrain-auc:0.958138\ttest-auc:0.945511\n",
      "[132]\ttrain-auc:0.958271\ttest-auc:0.94559\n",
      "[133]\ttrain-auc:0.958379\ttest-auc:0.94568\n",
      "[134]\ttrain-auc:0.958479\ttest-auc:0.945714\n",
      "[135]\ttrain-auc:0.958621\ttest-auc:0.945793\n",
      "[136]\ttrain-auc:0.958742\ttest-auc:0.945886\n",
      "[137]\ttrain-auc:0.958839\ttest-auc:0.945914\n",
      "[138]\ttrain-auc:0.959013\ttest-auc:0.946009\n",
      "[139]\ttrain-auc:0.959102\ttest-auc:0.946066\n",
      "[140]\ttrain-auc:0.95921\ttest-auc:0.946137\n",
      "[141]\ttrain-auc:0.959361\ttest-auc:0.946189\n",
      "[142]\ttrain-auc:0.95944\ttest-auc:0.946211\n",
      "[143]\ttrain-auc:0.959534\ttest-auc:0.946293\n",
      "[144]\ttrain-auc:0.959718\ttest-auc:0.946338\n",
      "[145]\ttrain-auc:0.959834\ttest-auc:0.946379\n",
      "[146]\ttrain-auc:0.959936\ttest-auc:0.946438\n",
      "[147]\ttrain-auc:0.960092\ttest-auc:0.946524\n",
      "[148]\ttrain-auc:0.960215\ttest-auc:0.946566\n",
      "[149]\ttrain-auc:0.960316\ttest-auc:0.946599\n",
      "[150]\ttrain-auc:0.960402\ttest-auc:0.946685\n",
      "[151]\ttrain-auc:0.960502\ttest-auc:0.946749\n",
      "[152]\ttrain-auc:0.960597\ttest-auc:0.946801\n",
      "[153]\ttrain-auc:0.960686\ttest-auc:0.946855\n",
      "[154]\ttrain-auc:0.960795\ttest-auc:0.946849\n",
      "[155]\ttrain-auc:0.960869\ttest-auc:0.946858\n",
      "[156]\ttrain-auc:0.960981\ttest-auc:0.946953\n",
      "[157]\ttrain-auc:0.961035\ttest-auc:0.946995\n",
      "[158]\ttrain-auc:0.961151\ttest-auc:0.947111\n",
      "[159]\ttrain-auc:0.961204\ttest-auc:0.947154\n",
      "[160]\ttrain-auc:0.961266\ttest-auc:0.947147\n",
      "[161]\ttrain-auc:0.961345\ttest-auc:0.947178\n",
      "[162]\ttrain-auc:0.961434\ttest-auc:0.947192\n",
      "[163]\ttrain-auc:0.961674\ttest-auc:0.947366\n",
      "[164]\ttrain-auc:0.961751\ttest-auc:0.947432\n",
      "[165]\ttrain-auc:0.96185\ttest-auc:0.947487\n",
      "[166]\ttrain-auc:0.961962\ttest-auc:0.947489\n",
      "[167]\ttrain-auc:0.962095\ttest-auc:0.947523\n",
      "[168]\ttrain-auc:0.962195\ttest-auc:0.947536\n",
      "[169]\ttrain-auc:0.962261\ttest-auc:0.947593\n",
      "[170]\ttrain-auc:0.962346\ttest-auc:0.947644\n",
      "[171]\ttrain-auc:0.962464\ttest-auc:0.947669\n",
      "[172]\ttrain-auc:0.9626\ttest-auc:0.94774\n",
      "[173]\ttrain-auc:0.962689\ttest-auc:0.947783\n",
      "[174]\ttrain-auc:0.962802\ttest-auc:0.947825\n",
      "[175]\ttrain-auc:0.962895\ttest-auc:0.947889\n",
      "[176]\ttrain-auc:0.96297\ttest-auc:0.947955\n",
      "[177]\ttrain-auc:0.963024\ttest-auc:0.947931\n",
      "[178]\ttrain-auc:0.963134\ttest-auc:0.948008\n",
      "[179]\ttrain-auc:0.963216\ttest-auc:0.948063\n",
      "[180]\ttrain-auc:0.963271\ttest-auc:0.9481\n",
      "[181]\ttrain-auc:0.963346\ttest-auc:0.948141\n",
      "[182]\ttrain-auc:0.963413\ttest-auc:0.948155\n",
      "[183]\ttrain-auc:0.963486\ttest-auc:0.948167\n",
      "[184]\ttrain-auc:0.96355\ttest-auc:0.948199\n",
      "[185]\ttrain-auc:0.963611\ttest-auc:0.948202\n",
      "[186]\ttrain-auc:0.963689\ttest-auc:0.948243\n",
      "[187]\ttrain-auc:0.963745\ttest-auc:0.948281\n",
      "[188]\ttrain-auc:0.963836\ttest-auc:0.948338\n",
      "[189]\ttrain-auc:0.963916\ttest-auc:0.948375\n",
      "[190]\ttrain-auc:0.964007\ttest-auc:0.948402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[191]\ttrain-auc:0.964086\ttest-auc:0.948474\n",
      "[192]\ttrain-auc:0.964146\ttest-auc:0.948512\n",
      "[193]\ttrain-auc:0.964247\ttest-auc:0.948545\n",
      "[194]\ttrain-auc:0.964324\ttest-auc:0.948607\n",
      "[195]\ttrain-auc:0.964385\ttest-auc:0.948671\n",
      "[196]\ttrain-auc:0.964458\ttest-auc:0.948699\n",
      "[197]\ttrain-auc:0.964528\ttest-auc:0.948717\n",
      "[198]\ttrain-auc:0.964589\ttest-auc:0.948772\n",
      "[199]\ttrain-auc:0.964675\ttest-auc:0.948809\n",
      "[200]\ttrain-auc:0.96475\ttest-auc:0.948891\n",
      "[201]\ttrain-auc:0.964819\ttest-auc:0.948969\n",
      "[202]\ttrain-auc:0.964888\ttest-auc:0.948978\n",
      "[203]\ttrain-auc:0.96494\ttest-auc:0.948986\n",
      "[204]\ttrain-auc:0.965019\ttest-auc:0.948971\n",
      "[205]\ttrain-auc:0.9651\ttest-auc:0.948986\n",
      "[206]\ttrain-auc:0.965138\ttest-auc:0.94902\n",
      "[207]\ttrain-auc:0.965201\ttest-auc:0.949003\n",
      "[208]\ttrain-auc:0.965271\ttest-auc:0.948995\n",
      "[209]\ttrain-auc:0.965306\ttest-auc:0.949015\n",
      "[210]\ttrain-auc:0.965376\ttest-auc:0.949057\n",
      "[211]\ttrain-auc:0.965423\ttest-auc:0.949077\n",
      "[212]\ttrain-auc:0.965475\ttest-auc:0.949127\n",
      "[213]\ttrain-auc:0.965544\ttest-auc:0.949152\n",
      "[214]\ttrain-auc:0.965614\ttest-auc:0.9492\n",
      "[215]\ttrain-auc:0.965689\ttest-auc:0.949257\n",
      "[216]\ttrain-auc:0.965768\ttest-auc:0.949279\n",
      "[217]\ttrain-auc:0.965848\ttest-auc:0.949325\n",
      "[218]\ttrain-auc:0.965898\ttest-auc:0.949352\n",
      "[219]\ttrain-auc:0.965969\ttest-auc:0.949353\n",
      "[220]\ttrain-auc:0.966051\ttest-auc:0.949387\n",
      "[221]\ttrain-auc:0.96611\ttest-auc:0.949359\n",
      "[222]\ttrain-auc:0.966169\ttest-auc:0.949399\n",
      "[223]\ttrain-auc:0.96625\ttest-auc:0.949439\n",
      "[224]\ttrain-auc:0.966318\ttest-auc:0.949571\n",
      "[225]\ttrain-auc:0.966369\ttest-auc:0.94963\n",
      "[226]\ttrain-auc:0.966427\ttest-auc:0.949651\n",
      "[227]\ttrain-auc:0.966513\ttest-auc:0.949699\n",
      "[228]\ttrain-auc:0.966582\ttest-auc:0.949749\n",
      "[229]\ttrain-auc:0.966629\ttest-auc:0.949793\n",
      "[230]\ttrain-auc:0.966687\ttest-auc:0.94977\n",
      "[231]\ttrain-auc:0.966767\ttest-auc:0.949803\n",
      "[232]\ttrain-auc:0.966842\ttest-auc:0.949811\n",
      "[233]\ttrain-auc:0.966923\ttest-auc:0.949836\n",
      "[234]\ttrain-auc:0.96699\ttest-auc:0.949882\n",
      "[235]\ttrain-auc:0.967044\ttest-auc:0.949919\n",
      "[236]\ttrain-auc:0.967083\ttest-auc:0.949949\n",
      "[237]\ttrain-auc:0.967173\ttest-auc:0.949964\n",
      "[238]\ttrain-auc:0.967223\ttest-auc:0.94996\n",
      "[239]\ttrain-auc:0.967304\ttest-auc:0.949968\n",
      "[240]\ttrain-auc:0.967355\ttest-auc:0.949957\n",
      "[241]\ttrain-auc:0.967447\ttest-auc:0.950005\n",
      "[242]\ttrain-auc:0.967561\ttest-auc:0.950057\n",
      "[243]\ttrain-auc:0.967648\ttest-auc:0.95008\n",
      "[244]\ttrain-auc:0.967767\ttest-auc:0.950094\n",
      "[245]\ttrain-auc:0.967849\ttest-auc:0.95017\n",
      "[246]\ttrain-auc:0.967951\ttest-auc:0.950193\n",
      "[247]\ttrain-auc:0.968009\ttest-auc:0.950209\n",
      "[248]\ttrain-auc:0.968061\ttest-auc:0.950211\n",
      "[249]\ttrain-auc:0.968111\ttest-auc:0.950234\n",
      "[250]\ttrain-auc:0.968173\ttest-auc:0.950214\n",
      "[251]\ttrain-auc:0.968233\ttest-auc:0.950231\n",
      "[252]\ttrain-auc:0.968298\ttest-auc:0.950198\n",
      "[253]\ttrain-auc:0.968355\ttest-auc:0.950211\n",
      "[254]\ttrain-auc:0.968419\ttest-auc:0.950231\n",
      "[255]\ttrain-auc:0.968481\ttest-auc:0.950282\n",
      "[256]\ttrain-auc:0.968532\ttest-auc:0.950305\n",
      "[257]\ttrain-auc:0.96863\ttest-auc:0.950337\n",
      "[258]\ttrain-auc:0.968675\ttest-auc:0.950348\n",
      "[259]\ttrain-auc:0.968723\ttest-auc:0.950356\n",
      "[260]\ttrain-auc:0.96877\ttest-auc:0.950356\n",
      "[261]\ttrain-auc:0.968833\ttest-auc:0.950376\n",
      "[262]\ttrain-auc:0.968879\ttest-auc:0.950378\n",
      "[263]\ttrain-auc:0.968928\ttest-auc:0.950413\n",
      "[264]\ttrain-auc:0.968993\ttest-auc:0.950459\n",
      "[265]\ttrain-auc:0.969074\ttest-auc:0.950485\n",
      "[266]\ttrain-auc:0.969109\ttest-auc:0.950499\n",
      "[267]\ttrain-auc:0.969166\ttest-auc:0.95052\n",
      "[268]\ttrain-auc:0.969224\ttest-auc:0.950526\n",
      "[269]\ttrain-auc:0.969283\ttest-auc:0.95055\n",
      "[270]\ttrain-auc:0.969341\ttest-auc:0.95057\n",
      "[271]\ttrain-auc:0.969376\ttest-auc:0.950588\n",
      "[272]\ttrain-auc:0.969444\ttest-auc:0.950607\n",
      "[273]\ttrain-auc:0.969483\ttest-auc:0.950634\n",
      "[274]\ttrain-auc:0.969583\ttest-auc:0.950643\n",
      "[275]\ttrain-auc:0.969633\ttest-auc:0.950658\n",
      "[276]\ttrain-auc:0.969663\ttest-auc:0.950721\n",
      "[277]\ttrain-auc:0.969717\ttest-auc:0.950715\n",
      "[278]\ttrain-auc:0.969748\ttest-auc:0.950718\n",
      "[279]\ttrain-auc:0.969799\ttest-auc:0.950712\n",
      "[280]\ttrain-auc:0.969837\ttest-auc:0.950705\n",
      "[281]\ttrain-auc:0.969932\ttest-auc:0.950709\n",
      "[282]\ttrain-auc:0.969997\ttest-auc:0.950789\n",
      "[283]\ttrain-auc:0.970063\ttest-auc:0.950832\n",
      "[284]\ttrain-auc:0.970108\ttest-auc:0.950834\n",
      "[285]\ttrain-auc:0.970153\ttest-auc:0.95086\n",
      "[286]\ttrain-auc:0.970193\ttest-auc:0.950865\n",
      "[287]\ttrain-auc:0.970244\ttest-auc:0.95088\n",
      "[288]\ttrain-auc:0.970299\ttest-auc:0.950935\n",
      "[289]\ttrain-auc:0.970384\ttest-auc:0.950954\n",
      "[290]\ttrain-auc:0.970454\ttest-auc:0.950978\n",
      "[291]\ttrain-auc:0.970491\ttest-auc:0.951009\n",
      "[292]\ttrain-auc:0.970525\ttest-auc:0.951021\n",
      "[293]\ttrain-auc:0.970585\ttest-auc:0.950997\n",
      "[294]\ttrain-auc:0.970642\ttest-auc:0.951055\n",
      "[295]\ttrain-auc:0.970684\ttest-auc:0.951092\n",
      "[296]\ttrain-auc:0.970739\ttest-auc:0.951129\n",
      "[297]\ttrain-auc:0.970776\ttest-auc:0.951139\n",
      "[298]\ttrain-auc:0.970816\ttest-auc:0.951144\n",
      "[299]\ttrain-auc:0.970873\ttest-auc:0.951139\n",
      "[300]\ttrain-auc:0.97092\ttest-auc:0.951158\n",
      "[301]\ttrain-auc:0.970976\ttest-auc:0.951174\n",
      "[302]\ttrain-auc:0.971023\ttest-auc:0.951131\n",
      "[303]\ttrain-auc:0.971093\ttest-auc:0.95113\n",
      "[304]\ttrain-auc:0.971155\ttest-auc:0.951116\n",
      "[305]\ttrain-auc:0.971232\ttest-auc:0.951119\n",
      "[306]\ttrain-auc:0.971258\ttest-auc:0.951152\n",
      "[307]\ttrain-auc:0.971312\ttest-auc:0.95115\n",
      "[308]\ttrain-auc:0.971358\ttest-auc:0.951195\n",
      "[309]\ttrain-auc:0.971418\ttest-auc:0.951196\n",
      "[310]\ttrain-auc:0.971473\ttest-auc:0.951232\n",
      "[311]\ttrain-auc:0.97153\ttest-auc:0.951227\n",
      "[312]\ttrain-auc:0.971588\ttest-auc:0.951218\n",
      "[313]\ttrain-auc:0.971636\ttest-auc:0.951239\n",
      "[314]\ttrain-auc:0.971681\ttest-auc:0.951266\n",
      "[315]\ttrain-auc:0.971714\ttest-auc:0.951292\n",
      "[316]\ttrain-auc:0.971765\ttest-auc:0.951296\n",
      "[317]\ttrain-auc:0.971792\ttest-auc:0.951313\n",
      "[318]\ttrain-auc:0.971841\ttest-auc:0.951318\n",
      "[319]\ttrain-auc:0.971883\ttest-auc:0.951325\n",
      "[320]\ttrain-auc:0.97193\ttest-auc:0.951326\n",
      "[321]\ttrain-auc:0.971993\ttest-auc:0.951368\n",
      "[322]\ttrain-auc:0.972057\ttest-auc:0.951361\n",
      "[323]\ttrain-auc:0.972126\ttest-auc:0.951403\n",
      "[324]\ttrain-auc:0.972178\ttest-auc:0.951417\n",
      "[325]\ttrain-auc:0.972207\ttest-auc:0.951417\n",
      "[326]\ttrain-auc:0.97224\ttest-auc:0.951427\n",
      "[327]\ttrain-auc:0.972284\ttest-auc:0.951445\n",
      "[328]\ttrain-auc:0.972342\ttest-auc:0.951463\n",
      "[329]\ttrain-auc:0.972371\ttest-auc:0.951461\n",
      "[330]\ttrain-auc:0.972417\ttest-auc:0.951448\n",
      "[331]\ttrain-auc:0.972456\ttest-auc:0.95146\n",
      "[332]\ttrain-auc:0.972503\ttest-auc:0.951454\n",
      "[333]\ttrain-auc:0.972551\ttest-auc:0.951481\n",
      "[334]\ttrain-auc:0.972573\ttest-auc:0.951492\n",
      "[335]\ttrain-auc:0.972635\ttest-auc:0.951488\n",
      "[336]\ttrain-auc:0.972673\ttest-auc:0.951487\n",
      "[337]\ttrain-auc:0.972741\ttest-auc:0.951491\n",
      "[338]\ttrain-auc:0.972779\ttest-auc:0.951497\n",
      "[339]\ttrain-auc:0.972862\ttest-auc:0.95147\n",
      "[340]\ttrain-auc:0.972898\ttest-auc:0.951465\n",
      "[341]\ttrain-auc:0.972949\ttest-auc:0.951472\n",
      "[342]\ttrain-auc:0.972995\ttest-auc:0.951487\n",
      "[343]\ttrain-auc:0.973037\ttest-auc:0.951488\n",
      "[344]\ttrain-auc:0.973115\ttest-auc:0.951517\n",
      "[345]\ttrain-auc:0.973147\ttest-auc:0.951536\n",
      "[346]\ttrain-auc:0.973193\ttest-auc:0.95155\n",
      "[347]\ttrain-auc:0.973222\ttest-auc:0.951559\n",
      "[348]\ttrain-auc:0.97326\ttest-auc:0.951549\n",
      "[349]\ttrain-auc:0.973293\ttest-auc:0.951548\n",
      "[350]\ttrain-auc:0.973347\ttest-auc:0.951548\n",
      "[351]\ttrain-auc:0.973397\ttest-auc:0.951553\n",
      "[352]\ttrain-auc:0.973421\ttest-auc:0.951558\n",
      "[353]\ttrain-auc:0.97348\ttest-auc:0.951557\n",
      "[354]\ttrain-auc:0.973555\ttest-auc:0.951595\n",
      "[355]\ttrain-auc:0.973593\ttest-auc:0.951619\n",
      "[356]\ttrain-auc:0.973678\ttest-auc:0.951593\n",
      "[357]\ttrain-auc:0.973707\ttest-auc:0.951609\n",
      "[358]\ttrain-auc:0.973759\ttest-auc:0.951605\n",
      "[359]\ttrain-auc:0.973792\ttest-auc:0.951611\n",
      "[360]\ttrain-auc:0.973842\ttest-auc:0.951625\n",
      "[361]\ttrain-auc:0.973887\ttest-auc:0.951691\n",
      "[362]\ttrain-auc:0.973951\ttest-auc:0.951687\n",
      "[363]\ttrain-auc:0.974008\ttest-auc:0.951681\n",
      "[364]\ttrain-auc:0.974049\ttest-auc:0.951712\n",
      "[365]\ttrain-auc:0.974071\ttest-auc:0.951725\n",
      "[366]\ttrain-auc:0.974103\ttest-auc:0.951734\n",
      "[367]\ttrain-auc:0.974138\ttest-auc:0.951742\n",
      "[368]\ttrain-auc:0.974178\ttest-auc:0.951758\n",
      "[369]\ttrain-auc:0.974225\ttest-auc:0.951807\n",
      "[370]\ttrain-auc:0.974262\ttest-auc:0.951808\n",
      "[371]\ttrain-auc:0.9743\ttest-auc:0.951857\n",
      "[372]\ttrain-auc:0.974346\ttest-auc:0.951855\n",
      "[373]\ttrain-auc:0.974379\ttest-auc:0.951861\n",
      "[374]\ttrain-auc:0.974419\ttest-auc:0.951871\n",
      "[375]\ttrain-auc:0.974449\ttest-auc:0.951857\n",
      "[376]\ttrain-auc:0.974479\ttest-auc:0.951856\n",
      "[377]\ttrain-auc:0.974501\ttest-auc:0.95185\n",
      "[378]\ttrain-auc:0.974537\ttest-auc:0.951859\n",
      "[379]\ttrain-auc:0.974573\ttest-auc:0.951846\n",
      "[380]\ttrain-auc:0.974619\ttest-auc:0.951889\n",
      "[381]\ttrain-auc:0.974691\ttest-auc:0.95189\n",
      "[382]\ttrain-auc:0.974741\ttest-auc:0.951867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[383]\ttrain-auc:0.974787\ttest-auc:0.951883\n",
      "[384]\ttrain-auc:0.97484\ttest-auc:0.951902\n",
      "[385]\ttrain-auc:0.974869\ttest-auc:0.951928\n",
      "[386]\ttrain-auc:0.974904\ttest-auc:0.951986\n",
      "[387]\ttrain-auc:0.974943\ttest-auc:0.951956\n",
      "[388]\ttrain-auc:0.975009\ttest-auc:0.951978\n",
      "[389]\ttrain-auc:0.975031\ttest-auc:0.952013\n",
      "[390]\ttrain-auc:0.975133\ttest-auc:0.95217\n",
      "[391]\ttrain-auc:0.97517\ttest-auc:0.952171\n",
      "[392]\ttrain-auc:0.975197\ttest-auc:0.952203\n",
      "[393]\ttrain-auc:0.975231\ttest-auc:0.952229\n",
      "[394]\ttrain-auc:0.975278\ttest-auc:0.952215\n",
      "[395]\ttrain-auc:0.975334\ttest-auc:0.952232\n",
      "[396]\ttrain-auc:0.975365\ttest-auc:0.952237\n",
      "[397]\ttrain-auc:0.975401\ttest-auc:0.952227\n",
      "[398]\ttrain-auc:0.975443\ttest-auc:0.952234\n",
      "[399]\ttrain-auc:0.97548\ttest-auc:0.952243\n",
      "[400]\ttrain-auc:0.975515\ttest-auc:0.952248\n",
      "[401]\ttrain-auc:0.975544\ttest-auc:0.952241\n",
      "[402]\ttrain-auc:0.975574\ttest-auc:0.952263\n",
      "[403]\ttrain-auc:0.975608\ttest-auc:0.952276\n",
      "[404]\ttrain-auc:0.97565\ttest-auc:0.952266\n",
      "[405]\ttrain-auc:0.975673\ttest-auc:0.952254\n",
      "[406]\ttrain-auc:0.975732\ttest-auc:0.952274\n",
      "[407]\ttrain-auc:0.975765\ttest-auc:0.952307\n",
      "[408]\ttrain-auc:0.975836\ttest-auc:0.952351\n",
      "[409]\ttrain-auc:0.975877\ttest-auc:0.95235\n",
      "[410]\ttrain-auc:0.975916\ttest-auc:0.952398\n",
      "[411]\ttrain-auc:0.975956\ttest-auc:0.95241\n",
      "[412]\ttrain-auc:0.975981\ttest-auc:0.952428\n",
      "[413]\ttrain-auc:0.976006\ttest-auc:0.952405\n",
      "[414]\ttrain-auc:0.976039\ttest-auc:0.952394\n",
      "[415]\ttrain-auc:0.976085\ttest-auc:0.952427\n",
      "[416]\ttrain-auc:0.976127\ttest-auc:0.952443\n",
      "[417]\ttrain-auc:0.976144\ttest-auc:0.952467\n",
      "[418]\ttrain-auc:0.976189\ttest-auc:0.952471\n",
      "[419]\ttrain-auc:0.976212\ttest-auc:0.952474\n",
      "[420]\ttrain-auc:0.976263\ttest-auc:0.952495\n",
      "[421]\ttrain-auc:0.976293\ttest-auc:0.952488\n",
      "[422]\ttrain-auc:0.976311\ttest-auc:0.952465\n",
      "[423]\ttrain-auc:0.976339\ttest-auc:0.952449\n",
      "[424]\ttrain-auc:0.976365\ttest-auc:0.952421\n",
      "[425]\ttrain-auc:0.976417\ttest-auc:0.952442\n",
      "[426]\ttrain-auc:0.976449\ttest-auc:0.952445\n",
      "[427]\ttrain-auc:0.976485\ttest-auc:0.952475\n",
      "[428]\ttrain-auc:0.976519\ttest-auc:0.952476\n",
      "[429]\ttrain-auc:0.976556\ttest-auc:0.952495\n",
      "[430]\ttrain-auc:0.976596\ttest-auc:0.952505\n",
      "[431]\ttrain-auc:0.976624\ttest-auc:0.952507\n",
      "[432]\ttrain-auc:0.976683\ttest-auc:0.95246\n",
      "[433]\ttrain-auc:0.976708\ttest-auc:0.952445\n",
      "[434]\ttrain-auc:0.976747\ttest-auc:0.952454\n",
      "[435]\ttrain-auc:0.976791\ttest-auc:0.952419\n",
      "[436]\ttrain-auc:0.976824\ttest-auc:0.9524\n",
      "[437]\ttrain-auc:0.976861\ttest-auc:0.952402\n",
      "[438]\ttrain-auc:0.976898\ttest-auc:0.952397\n",
      "[439]\ttrain-auc:0.976925\ttest-auc:0.952413\n",
      "[440]\ttrain-auc:0.976964\ttest-auc:0.952428\n",
      "[441]\ttrain-auc:0.976987\ttest-auc:0.952445\n",
      "[442]\ttrain-auc:0.977009\ttest-auc:0.952452\n",
      "[443]\ttrain-auc:0.977037\ttest-auc:0.952452\n",
      "[444]\ttrain-auc:0.977071\ttest-auc:0.952424\n",
      "[445]\ttrain-auc:0.977116\ttest-auc:0.952449\n",
      "[446]\ttrain-auc:0.977163\ttest-auc:0.952438\n",
      "[447]\ttrain-auc:0.977189\ttest-auc:0.952458\n",
      "[448]\ttrain-auc:0.977214\ttest-auc:0.952479\n",
      "[449]\ttrain-auc:0.97724\ttest-auc:0.952503\n",
      "[450]\ttrain-auc:0.977282\ttest-auc:0.9525\n",
      "[451]\ttrain-auc:0.977313\ttest-auc:0.952482\n",
      "Stopping. Best iteration:\n",
      "[431]\ttrain-auc:0.976624\ttest-auc:0.952507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train the model')\n",
    "xgtrain, xgvalid, model_params = build_xgb(input_train, y_train ,input_valid, y_val)\n",
    "model = train_xgboost(xgtrain, xgvalid, model_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_hat = np.zeros(( X_val.shape[0], 1) )\n",
    "validate_hat[:,0] = model.predict(xgb.DMatrix(input_valid), ntree_limit=model.best_ntree_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6209603699521428 for threshold: 0.2\n",
      "best threshold to generate predictions:  0.2\n",
      "best score:  0.6209603699521428\n",
      "F1 score: 0.6203346203346204 for threshold: 0.3\n",
      "best threshold to generate predictions:  0.2\n",
      "best score:  0.6209603699521428\n",
      "F1 score: 0.6189679737347458 for threshold: 0.31\n",
      "best threshold to generate predictions:  0.2\n",
      "best score:  0.6209603699521428\n",
      "F1 score: 0.6160849772382399 for threshold: 0.33\n",
      "best threshold to generate predictions:  0.2\n",
      "best score:  0.6209603699521428\n",
      "F1 score: 0.5994295028524858 for threshold: 0.4\n",
      "best threshold to generate predictions:  0.2\n",
      "best score:  0.6209603699521428\n",
      "F1 score: 0.5784676730276274 for threshold: 0.45\n",
      "best threshold to generate predictions:  0.2\n",
      "best score:  0.6209603699521428\n",
      "F1 score: 0.5581947743467933 for threshold: 0.5\n",
      "best threshold to generate predictions:  0.2\n",
      "best score:  0.6209603699521428\n"
     ]
    }
   ],
   "source": [
    "scores_list = []\n",
    "for threshold in [0.2, 0.3, 0.31, 0.33, 0.4, 0.45, 0.5]:\n",
    "    score = f1_score(y_val, (validate_hat > threshold).astype(int))\n",
    "    scores_list.append([threshold, score])\n",
    "    print('F1 score: {} for threshold: {}'.format(score, threshold))\n",
    "        \n",
    "    scores_list.sort(key=lambda x:x[1] , reverse=True)\n",
    "    best_threshold = scores_list[0][0]\n",
    "    print('best threshold to generate predictions: ', best_threshold)\n",
    "    print('best score: ', scores_list[0][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting.......\n",
      "Training F1 score:  0.9973402566617047\n",
      "Training Accuracy:  0.9996701246701246\n",
      "Testing F1 score:  0.5956762921872569\n",
      "Testing Accuracy:  0.952231328016856\n"
     ]
    }
   ],
   "source": [
    "# using kfold technique for fitting and predicting using Random Forest Classifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "test_preds = 0\n",
    "oof_preds = np.zeros([train_df.shape[0],])\n",
    "\n",
    "classifier5 = RandomForestClassifier()\n",
    "print('fitting.......')\n",
    "classifier5.fit(train_text_features_tf,train_target)\n",
    "pred_train=classifier5.predict_proba(train_text_features_tf)[:,1]\n",
    "# training data F1 score\n",
    "pred_train = np.where(pred_train > 0.25, 1, 0)\n",
    "print(\"Training F1 score: \",f1_score(train_target, pred_train))\n",
    "\n",
    "# training data accuracy score\n",
    "print(\"Training Accuracy: \",accuracy_score(train_target, pred_train))\n",
    "\n",
    "pred_test=classifier5.predict_proba(test_text_features_tf)[:,1]\n",
    "pred_test = np.where(pred_test > 0.25, 1, 0)\n",
    "print(\"Testing F1 score: \",f1_score(test_target, pred_test))\n",
    "\n",
    "# training data accuracy score\n",
    "print(\"Testing Accuracy: \",accuracy_score(test_target, pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
